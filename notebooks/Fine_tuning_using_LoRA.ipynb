{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d726b890",
   "metadata": {},
   "source": [
    "# Fine Tune Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92463bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\work_space\\projects\\deep_learning\\CAP\\lib\\site-packages\\xformers\\__init__.py\", line 57, in _is_triton_available\n",
      "    import triton  # noqa\n",
      "ModuleNotFoundError: No module named 'triton'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from transformers import AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d9b3a",
   "metadata": {},
   "source": [
    "## Configuratin & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "474386e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "data_dir = r\"D:\\work_space\\projects\\deep_learning\\CAP6415_F25_project-Finding-and-solving-hard-to-generate-examples\\Data_set\\processed\"   \n",
    "resolution = 512         \n",
    "train_batch_size = 1\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 5e-4\n",
    "num_epochs = 10           # will be cut off by max_train_steps\n",
    "max_train_steps = 300  # total update steps (not epochs)\n",
    "lora_rank = 2             # safe for 6GB\n",
    "num_workers = 0           # Windows + small batch = 0 is safest\n",
    "seed = 42\n",
    "\n",
    "save_dir = r\"D:\\work_space\\projects\\deep_learning\\CAP6415_F25_project-Finding-and-solving-hard-to-generate-examples\\model\\lora_output\"    \n",
    "CHECKPOINT_DIR = save_dir + r\"\\checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4bbf024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    print(\"Seed set to\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ee9c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size (images with captions): 222\n"
     ]
    }
   ],
   "source": [
    "# DATASET\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, folder: str | Path, res: int):\n",
    "        self.items = []\n",
    "        self.res = res\n",
    "        folder = Path(folder)\n",
    "\n",
    "        for img_path in sorted(folder.glob(\"*.jpg\")):\n",
    "            txt_path = img_path.with_suffix(\".txt\")\n",
    "            if txt_path.exists():\n",
    "                with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    lines = [ln.strip() for ln in f.readlines() if ln.strip()]\n",
    "                if not lines:\n",
    "                    continue\n",
    "                self.items.append((str(img_path), lines))\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((res, res)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.5]*3, [0.5]*3),\n",
    "        ])\n",
    "\n",
    "        print(\"Dataset size (images with captions):\", len(self.items))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, captions = self.items[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "\n",
    "        # randomly choose one caption line for this pass\n",
    "        caption = random.choice(captions)\n",
    "        return img, caption\n",
    "\n",
    "\n",
    "dataset = CaptionDataset(data_dir, resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a283ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA MODULES\n",
    "def get_parent(model, name: str):\n",
    "    parts = name.split(\".\")\n",
    "    obj = model\n",
    "    for p in parts[:-1]:\n",
    "        obj = getattr(obj, p)\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96f233dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, original: nn.Linear, rank: int = 2):\n",
    "        super().__init__()\n",
    "        self.original = original\n",
    "        in_f = original.in_features\n",
    "        out_f = original.out_features\n",
    "\n",
    "        self.lora_A = nn.Linear(in_f, rank, bias=False)\n",
    "        self.lora_B = nn.Linear(rank, out_f, bias=False)\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=5 ** 0.5)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        self.scale = 1.0 / rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.original(x) + self.scale * self.lora_B(self.lora_A(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3ef0c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_lora(unet: nn.Module, rank: int = 2):\n",
    "    count = 0\n",
    "    for name, module in list(unet.named_modules()):\n",
    "        if isinstance(module, nn.Linear) and any(k in name for k in [\"to_q\", \"to_k\", \"to_v\"]):\n",
    "            parent = get_parent(unet, name)\n",
    "            child_name = name.split(\".\")[-1]\n",
    "            setattr(parent, child_name, LoRALinear(module, rank=rank))\n",
    "            count += 1\n",
    "    print(\"Injected LoRA into\", count, \"linear layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdf7815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3602e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATS of gpu\n",
    "def print_cuda_stats():\n",
    "    print(f\"VRAM Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "    print(f\"VRAM Reserved:  {torch.cuda.memory_reserved()/1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a4ea4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train_lora():\n",
    "    set_seed(seed)\n",
    "    accelerator = Accelerator()  \n",
    "    device = accelerator.device\n",
    "    print(\"Accelerator device:\", device)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # LOAD PIPE\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        safety_checker=None,\n",
    "        torch_dtype=torch.float32,\n",
    "    )\n",
    "    pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
    "    pipe.to(device)\n",
    "    print(\"Pipeline loaded\")\n",
    "\n",
    "    # EXTRACT COMPONENTS\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
    "    unet = pipe.unet\n",
    "    vae = pipe.vae\n",
    "    text_encoder = pipe.text_encoder\n",
    "\n",
    "    # GRADIENT CHECKPOINTING + FREEZE BASE WEIGHTS\n",
    "    unet.enable_gradient_checkpointing()\n",
    "    if hasattr(text_encoder, \"gradient_checkpointing_enable\"):\n",
    "        text_encoder.gradient_checkpointing_enable()\n",
    "\n",
    "    for p in unet.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in text_encoder.parameters():\n",
    "        p.requires_grad = False  # we train only UNet LoRA\n",
    "\n",
    "    print(\"UNet and text encoder frozen (base)\")\n",
    "\n",
    "    # ADD LORA\n",
    "    inject_lora(unet, lora_rank)\n",
    "\n",
    "    # COLLECT TRAINABLE PARAMS (ONLY LoRA)\n",
    "    trainable_params = [p for p in unet.parameters() if p.requires_grad]\n",
    "    print(\"Trainable parameters (LoRA only):\", sum(p.numel() for p in trainable_params))\n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=learning_rate)\n",
    "\n",
    "    # DATALOADER\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    unet, optimizer, loader = accelerator.prepare(unet, optimizer, loader)\n",
    "    print(\"Training start\")\n",
    "\n",
    "# TRAIN LOOP \n",
    "    steps = 0\n",
    "    unet.train()\n",
    "\n",
    "    print(\"\\n Debug mode enabled — watch first few steps carefully!\\n\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for img, caption in loader:\n",
    "\n",
    "            # Stop if max steps reached\n",
    "            if steps >= max_train_steps:\n",
    "                print(\"Stopped after\", steps, \"steps\")\n",
    "                pipe.unet = accelerator.unwrap_model(unet)\n",
    "                return pipe\n",
    "\n",
    "            # DEBUG (FIRST BATCH ONLY)\n",
    "            if steps == 0:\n",
    "                print(\"[debug] First caption:\", caption)\n",
    "                print(\"[debug] Image tensor shape:\", img.shape)\n",
    "\n",
    "            img = img.to(device)\n",
    "\n",
    "            # TOKENIZE CAPTION \n",
    "            tokens = tokenizer(\n",
    "                list(caption),\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            if steps == 0:   # debug only first iteration\n",
    "                print(\"[debug] Tokenizer output IDs shape:\", tokens.input_ids.shape)\n",
    "                print(\"[debug] Example input IDs:\", tokens.input_ids[0][:10])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                latents = vae.encode(img).latent_dist.sample() * vae.config.scaling_factor\n",
    "                enc = text_encoder(tokens.input_ids)[0]\n",
    "\n",
    "            noise = torch.randn_like(latents)\n",
    "            t = torch.randint(0, pipe.scheduler.config.num_train_timesteps,\n",
    "                            (latents.shape[0],), device=device, dtype=torch.long)\n",
    "            noisy = pipe.scheduler.add_noise(latents, noise, t)\n",
    "\n",
    "            pred = unet(noisy, t, enc).sample\n",
    "            loss = nn.functional.mse_loss(pred, noise)\n",
    "\n",
    "            # SAVE MODLES\n",
    "            if steps > 0 and steps % 50 == 0 and accelerator.is_main_process:\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"Running test inference at step: {steps}\")\n",
    "                with torch.no_grad():\n",
    "                    lora_dict = accelerator.unwrap_model(unet).state_dict()\n",
    "                    torch.save(lora_dict, f\"{CHECKPOINT_DIR}/step_{steps}.pt\")\n",
    "                    print(f\"Saved checkpoint: step_{steps}.pt\")\n",
    "                    print(f\"Loss: {loss.item():.4f}\")\n",
    "                    print_cuda_stats()\n",
    "                    print(\"\\n\")\n",
    "                    \n",
    "            # BACKPROP \n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            steps += 1\n",
    "\n",
    "    pipe.unet = accelerator.unwrap_model(unet)\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e43680f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Accelerator device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d4961bc4204687b24d76dc0452a3ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline loaded\n",
      "UNet and text encoder frozen (base)\n",
      "Injected LoRA into 96 linear layers\n",
      "Trainable parameters (LoRA only): 298752\n",
      "Training start\n",
      "\n",
      " Debug mode enabled — watch first few steps carefully!\n",
      "\n",
      "[debug] First caption: ['a speed bump on a city street.']\n",
      "[debug] Image tensor shape: torch.Size([1, 3, 512, 512])\n",
      "[debug] Tokenizer output IDs shape: torch.Size([1, 77])\n",
      "[debug] Example input IDs: tensor([49406,   320,  4163, 15877,   525,   320,  1305,  2012,   269, 49407],\n",
      "       device='cuda:0')\n",
      "Running test inference at step: 50\n",
      "Saved checkpoint: step_50.pt\n",
      "Loss: 0.0810\n",
      "VRAM Allocated: 4400.38 MB\n",
      "VRAM Reserved:  4574.00 MB\n",
      "\n",
      "\n",
      "Running test inference at step: 100\n",
      "Saved checkpoint: step_100.pt\n",
      "Loss: 0.5787\n",
      "VRAM Allocated: 4400.38 MB\n",
      "VRAM Reserved:  4466.00 MB\n",
      "\n",
      "\n",
      "Running test inference at step: 150\n",
      "Saved checkpoint: step_150.pt\n",
      "Loss: 0.0209\n",
      "VRAM Allocated: 4400.38 MB\n",
      "VRAM Reserved:  4494.00 MB\n",
      "\n",
      "\n",
      "Running test inference at step: 200\n",
      "Saved checkpoint: step_200.pt\n",
      "Loss: 0.0216\n",
      "VRAM Allocated: 4400.38 MB\n",
      "VRAM Reserved:  4494.00 MB\n",
      "\n",
      "\n",
      "Running test inference at step: 250\n",
      "Saved checkpoint: step_250.pt\n",
      "Loss: 0.5121\n",
      "VRAM Allocated: 4400.38 MB\n",
      "VRAM Reserved:  4494.00 MB\n",
      "\n",
      "\n",
      "Stopped after 300 steps\n",
      "LoRA weights saved at: D:\\work_space\\projects\\deep_learning\\CAP6415_F25_project-Finding-and-solving-hard-to-generate-examples\\model\\lora_output\\speedbump_lora.pt\n"
     ]
    }
   ],
   "source": [
    "# RUN TRAINING\n",
    "if __name__ == \"__main__\":\n",
    "    pipe = train_lora()\n",
    "    #SAVE LORA WEIGHTS \n",
    "    lora_state_dict = {k: v.cpu() for k, v in pipe.unet.state_dict().items() if \"lora_\" in k}\n",
    "    out_path = os.path.join(save_dir, \"speedbump_lora.pt\")\n",
    "    torch.save(lora_state_dict, out_path)\n",
    "    print(\"LoRA weights saved at:\", out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p_CAP",
   "language": "python",
   "name": "cap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
