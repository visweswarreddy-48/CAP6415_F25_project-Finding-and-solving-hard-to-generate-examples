{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7e6504",
   "metadata": {},
   "source": [
    "## Testing and CLIP-Based Evaluation of LoRA Fine-Tuned Stable Diffusion\n",
    "\n",
    "This notebook performs inference using the LoRA fine-tuned Stable Diffusion\n",
    "model and evaluates the generated images using the CLIP score. Multiple\n",
    "speed-bump-focused prompts are used to generate images that remain within the\n",
    "training distribution. The generated images are saved to disk, and CLIP-based\n",
    "text–image similarity is computed for each prompt–image pair to quantify\n",
    "semantic alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a397d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from peft import PeftModel\n",
    "from pathlib import Path\n",
    "import json\n",
    "from PIL import Image\n",
    "import clip\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f7a8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS \n",
    "base_model = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "LORA_DIR = Path(r\"D:\\work_space\\projects\\deep_learning\\CAP6415_F25_project-Finding-and-solving-hard-to-generate-examples\\model\\lora_peft_checkpoint\")\n",
    "CLIP_OUT_PATH = Path(r\"D:\\work_space\\projects\\deep_learning\\CAP6415_F25_project-Finding-and-solving-hard-to-generate-examples\\results\\Metrics_json\")\n",
    "OUTPUT_DIR = Path(r\"D:\\work_space\\projects\\deep_learning\\CAP6415_F25_project-Finding-and-solving-hard-to-generate-examples\\results\\After\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7ee0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# DEVICE\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "print(\"Using dtype:\", dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27f0cf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:05<00:00,  1.03it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "# LOAD BASE SD\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=dtype,\n",
    "    safety_checker=None\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b8f1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD & MERGE LORA \n",
    "pipe.unet = PeftModel.from_pretrained(pipe.unet, LORA_DIR).merge_and_unload()\n",
    "pipe.set_progress_bar_config(disable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfcfb7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTIPLE TEST PROMPTS \n",
    "prompts = [\n",
    "    \"a close-up of a speed bump on an asphalt road surface, realistic photo\",\n",
    "    \"a detailed view of a speed bump on an asphalt road, realistic photo\",\n",
    "    \"a wide view of a speed bump on a residential asphalt road, realistic photo\",\n",
    "    \"a ground-level view of a yellow and black speed bump on an asphalt street\",\n",
    "    \"a clear view of a speed bump on a paved asphalt road, realistic photo\",\n",
    "\n",
    "    \"a speed bump on a wet asphalt road after rain, realistic photo\",\n",
    "    \"a speed bump on a dry asphalt road in bright daylight, realistic photo\",\n",
    "    \"a speed bump on an asphalt road under cloudy lighting, realistic photo\",\n",
    "    \"a speed bump on an asphalt road in soft evening light, realistic photo\",\n",
    "    \"a speed bump on an asphalt road under street lighting at night, realistic photo\",\n",
    "\n",
    "    \"a newly painted yellow and black speed bump on an asphalt road, realistic photo\",\n",
    "    \"a slightly worn speed bump on an asphalt street, realistic photo\",\n",
    "    \"a faded yellow speed bump on an asphalt road surface, realistic photo\",\n",
    "\n",
    "    \"a speed bump near a pedestrian crossing on an asphalt road, realistic photo\",\n",
    "    \"a speed bump in a residential asphalt street, realistic photo\",\n",
    "    \"a speed bump in a parking area on asphalt, realistic photo\",\n",
    "\n",
    "    \"a speed bump on a narrow asphalt road, realistic photo\",\n",
    "    \"a speed bump on a straight asphalt road, realistic photo\",\n",
    "    \"a speed bump on an empty asphalt road, realistic photo\",\n",
    "    \"a speed bump on an urban asphalt street, realistic photo\"\n",
    "]\n",
    "negative_prompt = (\n",
    "    \"blurry, low resolution, bad anatomy, distorted, deformed, extra objects, \"\n",
    "    \"cartoon, anime, painting, illustration, unrealistic, fake, CGI, pothole, \"\n",
    "    \"flat road, smooth road surface, no speed bump\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b88d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:08<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:07<00:00,  4.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# IMAGE GENERATION LOOP \n",
    "generated_images = []\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    print(f\"Generating {idx+1}/{len(prompts)}\")\n",
    "\n",
    "    image = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=7.5,\n",
    "        height=512,\n",
    "        width=512\n",
    "    ).images[0]\n",
    "\n",
    "    out_path = OUTPUT_DIR / f\"lora_result_{idx:03d}.png\"\n",
    "    image.save(out_path)\n",
    "    generated_images.append(out_path)\n",
    "# Display one sample image\n",
    "image.show(generated_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624a35d",
   "metadata": {},
   "source": [
    "### CLIP SCORE EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85bb66ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP scores saved to: D:\\work_space\\projects\\deep_learning\\CAP6415_F25_project-Finding-and-solving-hard-to-generate-examples\\results\\Metrics_json\n",
      "Average CLIP score: 0.29957275390625\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_scores = []\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    img_path = OUTPUT_DIR / f\"lora_result_{idx:03d}.png\"\n",
    "    image = preprocess(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "    text = clip.tokenize([prompt]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (image_features @ text_features.T).item()\n",
    "    clip_scores.append(similarity)\n",
    "# Save scores\n",
    "with open(CLIP_OUT_PATH/ \"clip_scores.json\", \"w\") as f:\n",
    "    json.dump(clip_scores, f)\n",
    "\n",
    "print(\"CLIP scores saved to:\", CLIP_OUT_PATH)\n",
    "print(\"Average CLIP score:\", sum(clip_scores) / len(clip_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f290e3e",
   "metadata": {},
   "source": [
    "### Code Explanation \n",
    "\n",
    "This notebook loads the base Stable Diffusion v1.5 model and merges the trained\n",
    "LoRA adapter into the UNet using the PEFT framework for efficient inference.\n",
    "The computing device and numerical precision are selected automatically based\n",
    "on hardware availability.\n",
    "\n",
    "Multiple speed-bump-focused evaluation prompts are defined along with a\n",
    "negative prompt to suppress visual artifacts. For each prompt, the fine-tuned\n",
    "model generates a 512×512 image, which is saved to disk for evaluation.\n",
    "\n",
    "For quantitative analysis, the pretrained CLIP model (ViT-B/32) is used to\n",
    "measure text–image alignment. Each generated image and its corresponding\n",
    "prompt are encoded using CLIP, and the cosine similarity between their\n",
    "embeddings is computed as the CLIP score.\n",
    "\n",
    "All CLIP scores are saved as a JSON file, and the average CLIP score is\n",
    "reported as an overall measure of how well the fine-tuned model aligns with\n",
    "the given text prompts.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lock_in",
   "language": "python",
   "name": "lockin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
