{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5a57c9",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4af497a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a37f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIG\n",
    "# Raw data_set location\n",
    "source_folders = [\n",
    "    Path(r\"D:\\data_set\\Speed_Bump\\bump_detection_dataset\\train\\bump\"),\n",
    "    Path(r\"D:\\data_set\\Speed_Bump\\bump_detection_dataset\\test\\bump\")\n",
    "]\n",
    "#Destination location\n",
    "BASE_DIR     = Path(r\"D:\\CAP6415_F25_project-Finding-and-solving-hard-to-generate-examples\\Data_set\")\n",
    "#Inside the Data_set\n",
    "UNIQUE_EXACT = BASE_DIR / \"unique\"\n",
    "UNIQUE_CLIP  = BASE_DIR / \"unique_clip\"\n",
    "POOR_DIR     = BASE_DIR / \"poor_blur_detection\"\n",
    "PROCESSED    = BASE_DIR / \"processed\"\n",
    "\n",
    "for folder in [UNIQUE_EXACT, UNIQUE_CLIP, POOR_DIR, PROCESSED]:\n",
    "    folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1782a707",
   "metadata": {},
   "source": [
    "## Removal of Duplicate Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff84d16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing exact duplicate images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2982/2982 [00:39<00:00, 76.01it/s]\n",
      "100%|██████████| 1277/1277 [00:19<00:00, 64.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique images after Hash filtering: 4178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# REMOVE EXACT DUPLICATES (MD5 HASH)\n",
    "def get_hash(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        return hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "print(\"Removing exact duplicate images\")\n",
    "hashes = set()\n",
    "unique_count = 0\n",
    "\n",
    "for folder in source_folders:\n",
    "    for img in tqdm(list(folder.glob(\"*.*\"))):\n",
    "        try:\n",
    "            h = get_hash(img)\n",
    "            if h not in hashes:\n",
    "                hashes.add(h)\n",
    "                shutil.copy(img, UNIQUE_EXACT / f\"img_{unique_count:05d}{img.suffix}\")\n",
    "                unique_count += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(f\"Unique images after Hash filtering: {unique_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6702ff4",
   "metadata": {},
   "source": [
    "## Removal of Visually Similar images using CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3530e684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP-based similarity filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP filtering: 100%|██████████| 4178/4178 [04:42<00:00, 14.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining images after CLIP: 354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# REMOVE VISUALLY SIMILAR (CLIP)\n",
    "print(\"CLIP-based similarity filtering\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def compute_embedding(path):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = model.get_image_features(**inputs)\n",
    "    emb = emb / emb.norm(p=2)\n",
    "    return emb.cpu().numpy()[0]\n",
    "\n",
    "image_paths = sorted(list(UNIQUE_EXACT.glob(\"*.*\")))\n",
    "embeddings = []\n",
    "SIM_THRESHOLD = 0.90\n",
    "\n",
    "for img_path in tqdm(image_paths, desc=\"CLIP filtering\"):\n",
    "    try:\n",
    "        emb = compute_embedding(img_path)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    if all(np.dot(emb, e) < SIM_THRESHOLD for e in embeddings):\n",
    "        embeddings.append(emb)\n",
    "        shutil.copy(img_path, UNIQUE_CLIP / img_path.name)\n",
    "\n",
    "print(\"Remaining images after CLIP:\", len(list(UNIQUE_CLIP.glob('*.*'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b1db2",
   "metadata": {},
   "source": [
    "## Resize Images for Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "972768f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess (resize + RGB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 354/354 [00:11<00:00, 31.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed images saved: 354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESS (resize + RGB)\n",
    "print(\"Preprocess (resize + RGB)\")\n",
    "RES = 512\n",
    "count = 0\n",
    "\n",
    "for img_path in tqdm(list(UNIQUE_CLIP.glob(\"*.*\"))):\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = img.resize((RES, RES))\n",
    "        out_path = PROCESSED / f\"speedbump_{count:05d}.jpg\"\n",
    "        img.save(out_path, quality=95)\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", img_path, e)\n",
    "\n",
    "print(f\"Processed images saved: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11045dc7",
   "metadata": {},
   "source": [
    "## Removal of Blur Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a173e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blur Filtering\n",
      "Blurry images detected: 132\n",
      "Final Training Images: 222\n",
      "Poor Images Moved: 132\n"
     ]
    }
   ],
   "source": [
    "# BLUR DETECTION (REMOVE POOR IMAGES)\n",
    "print(\"Blur Filtering\")\n",
    "def blur_score(path):\n",
    "    img = cv2.imread(path, 0)\n",
    "    if img is None:\n",
    "        return None\n",
    "    return cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "\n",
    "bad = []\n",
    "BLUR_THRESHOLD = 120\n",
    "\n",
    "for fname in os.listdir(PROCESSED):\n",
    "    if fname.endswith(\".jpg\"):\n",
    "        path = PROCESSED / fname\n",
    "        score = blur_score(str(path))\n",
    "        if score is None or score < BLUR_THRESHOLD:\n",
    "            bad.append(fname)\n",
    "\n",
    "print(\"Blurry images detected:\", len(bad))\n",
    "\n",
    "for fname in bad:\n",
    "    shutil.move(PROCESSED / fname, POOR_DIR / fname)\n",
    "\n",
    "print(\"Final Training Images:\", len(list(PROCESSED.glob('*.*'))))\n",
    "print(\"Poor Images Moved:\", len(bad))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_env (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
